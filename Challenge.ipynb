{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Room Occupancy Estimation using Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook aims to analyze and develop machine learning models for **Room Occupancy Estimation** using non-intrusive environmental sensors such as **temperature, light, sound, CO2, and PIR (Passive Infrared)**. The goal is to predict the number of occupants in a room based on sensor data.\n",
    "\n",
    "This project is developed as part of the **Machine Learning Challenge (Aprendizagem Computacional), 2025** at **DEI, FCT, University of Coimbra**. The objective of the challenge is to identify a real-world classification problem, apply simple machine learning models, and evaluate their effectiveness.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "The dataset used in this project is [**Room Occupancy Estimation**](https://archive.ics.uci.edu/dataset/864/room+occupancy+estimation), which consists of **10129 instances and 18 features**. The data was collected over four days in a controlled environment, where occupancy varied between **0 and 3 people**.\n",
    "\n",
    "- **Sensor Types Used:**\n",
    "  - **Temperature**\n",
    "  - **Light**\n",
    "  - **Sound**\n",
    "  - **CO2**\n",
    "  - **Passive Infrared (PIR)**\n",
    "\n",
    "- **Experimental Setup:**\n",
    "  - Data was collected in a **6m x 4.6m room**.\n",
    "  - **7 sensor nodes** were deployed, transmitting data every **30 seconds**.\n",
    "  - The PIR, CO2, and sound sensors required manual calibration.\n",
    "  - The ground truth was manually recorded.\n",
    "\n",
    "## Challenge Goals\n",
    "\n",
    "The primary goals of this challenge are:\n",
    "1. **Problem Identification:** Understand how environmental sensor data can be used for room occupancy estimation.\n",
    "2. **Data Analysis:** Explore the dataset, clean, and preprocess it.\n",
    "3. **Model Construction:** Implement at least **two simple machine learning models** (e.g., Decision Trees, K-Nearest Neighbors, Logistic Regression).\n",
    "4. **Evaluation Metrics:** Assess model performance using appropriate evaluation metrics.\n",
    "5. **Documentation & Submission:** Record findings, methodologies, and challenges faced.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Load and preprocess the dataset.\n",
    "2. Perform exploratory data analysis (EDA).\n",
    "3. Train and evaluate machine learning models.\n",
    "4. Compare model performance using visualization tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Column Descriptions\n",
    "| **Feature/Target**    | **Type**       | **Description**                                              | **Units**      |\n",
    "|-------------------------|---------------|--------------------------------------------------------------|---------------|\n",
    "| `Date`                 | Date          | Date when the data was recorded                              | YYYY/MM/DD    |\n",
    "| `Time`                 | Time          | Time when the data was recorded                              | HH:MM:SS      |\n",
    "| `S1_Temp` – `S4_Temp`  | Continuous    | Ambient temperature measured at different sensor locations   | °C            |\n",
    "| `S1_Light` – `S4_Light`| Integer       | Light intensity measured at different sensor locations       | Lux           |\n",
    "| `S1_Sound` – `S4_Sound`| Continuous    | Sound levels measured using an amplifier output read by ADC  | Volts         |\n",
    "| `S5_CO2`              | Integer       | Carbon dioxide (CO2) concentration in the room              | PPM           |\n",
    "| `S5_CO2_Slope`       | Continuous    | Rate of change (slope) of CO2 concentration over time       | -             |\n",
    "| `S6_PIR`, `S7_PIR`    | Binary/Integer | Motion detection using Passive Infrared (PIR) sensors       | - (0 = No motion, 1 = Motion detected) |\n",
    "| `Room_Occupancy_Count` | Integer        | Ground truth number of occupants in the room                | -             |\n",
    "\n",
    "\n",
    "**NOTE:** There are no missing (null) values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Required Libs: pip install scikit-learn pandas numpy scipy matplotlib seaborn\n",
    "\n",
    "For report:\n",
    "- DETERMINE PROBLEM, SOLUTION, OBJECTIVE;\n",
    "- BREAKDOWN OF THE DATASET, DESCRIPTION OF VARIABLES AND TARGETS, NUMBER OF NULL VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "#Converts hours, minutes, seconds to seconds for easier handling\n",
    "\n",
    "def hms_to_seconds(t):\n",
    "    h, m, s = [int(i) for i in str(t).split(':')]\n",
    "    return 3600*h + 60*m + s\n",
    "  \n",
    "# fetch dataset \n",
    "room_occupancy_estimation = fetch_ucirepo(id=864) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = room_occupancy_estimation.data.features\n",
    "y = room_occupancy_estimation.data.targets\n",
    "\n",
    "# Get number of target elements of the same category\n",
    "target_distribution = y.pivot_table(index=[\"Room_Occupancy_Count\"], aggfunc=\"size\")\n",
    "print(target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of data entries\n",
    "X.join(y).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a theme for Seaborn plots\n",
    "sns.set_theme()\n",
    "\n",
    "# Histogram of \"Room_Occupancy_Count\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[\"0\", \"1\", \"2\", \"3\"], y=target_distribution.values)\n",
    "plt.title(\"Room Occupancy Distribution\")\n",
    "plt.xlabel(\"Number of Occupants\")\n",
    "plt.ylabel(\"Occurences\")\n",
    "plt.show()\n",
    "\n",
    "# We can see the dataset is unbalanced. Ruh-roh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - Exploration Data Analysis\n",
    "\n",
    "1. Remove rows with missing data - since there are no null values in the dataset, this step can be skipped.\n",
    "2. Remove duplicate rows. Notice how the duplicate data is (mostly) constrained to late night / early mornings when the room is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strictly numeric feature names\n",
    "numeric_features = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "# Count and remove duplicates\n",
    "mask_duplicates = X.duplicated(subset=numeric_features, keep=\"first\")\n",
    "count_duplicates = mask_duplicates.sum()\n",
    "\n",
    "print(count_duplicates)\n",
    "\n",
    "X1 = X.drop_duplicates(subset=numeric_features).select_dtypes(include=\"number\")\n",
    "y1 = y[~mask_duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated \"Room_Occupancy_Count\" after removing duplicates\n",
    "target_distribution = y1.pivot_table(index=[\"Room_Occupancy_Count\"], aggfunc=\"size\")\n",
    "\n",
    "print(target_distribution)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[\"0\", \"1\", \"2\", \"3\"], y=target_distribution.values)\n",
    "plt.title(\"Room Occupancy Distribution\")\n",
    "plt.xlabel(\"Number of Occupants\")\n",
    "plt.ylabel(\"Occurences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"DateTime\"] = pd.to_datetime(X[\"Date\"] + \" \" + X[\"Time\"])\n",
    "X[\"Hour\"] = X[\"DateTime\"].dt.hour\n",
    "\n",
    "# Plot occupancy by hour\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x=X[\"Hour\"], y=y[\"Room_Occupancy_Count\"], palette=\"coolwarm\")\n",
    "plt.title(\"Occupancy Count by Hour of the Day\")\n",
    "plt.xlabel(\"Hour of the Day\")\n",
    "plt.ylabel(\"Occupancy Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Univariate Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping sensor variables\n",
    "sensor_groups = {\n",
    "    \"Temperature (°C)\": [\"S1_Temp\", \"S2_Temp\", \"S3_Temp\", \"S4_Temp\"],\n",
    "    \"Light (Lux)\": [\"S1_Light\", \"S2_Light\", \"S3_Light\", \"S4_Light\"],\n",
    "    \"Sound (Volts)\": [\"S1_Sound\", \"S2_Sound\", \"S3_Sound\", \"S4_Sound\"],\n",
    "    \"CO2 (PPM)\": [\"S5_CO2\", \"S5_CO2_Slope\"],\n",
    "}\n",
    "\n",
    "# Plot continuous variables with KDE plots\n",
    "fig, axes = plt.subplots(len(sensor_groups) + 1, 1, figsize=(10, 18))  # Extra subplot for PIR\n",
    "for ax, (group_name, features) in zip(axes[:-1], sensor_groups.items()):\n",
    "    for feature in features:\n",
    "        sns.kdeplot(X1[feature], ax=ax, label=feature, fill=True, alpha=0.4)\n",
    "    \n",
    "    ax.set_title(f\"{group_name} Distribution\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "\n",
    "# Separate plot for PIR Motion (binary data) using bar plots\n",
    "ax_pir = axes[-1]\n",
    "pir_data = X1[[\"S6_PIR\", \"S7_PIR\"]].melt(var_name=\"Sensor\", value_name=\"Value\")\n",
    "sns.countplot(x=\"Value\", hue=\"Sensor\", data=pir_data, ax=ax_pir)\n",
    "\n",
    "ax_pir.set_title(\"PIR Motion Detection Counts\")\n",
    "ax_pir.set_xlabel(\"Motion Detected (0 = No, 1 = Yes)\")\n",
    "ax_pir.set_ylabel(\"Count\")\n",
    "ax_pir.legend(title=\"Sensor\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1y1 = X1.join(y1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(X1y1.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X1y1.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now study different sampling methods in order to determine the method that provides the most accuracy after training.\n",
    "\n",
    "## Case studies:\n",
    "- Baseline (no sampling method applied)\n",
    "- Oversampling\n",
    "    - RandomOverSampler\n",
    "    - SMOTE\n",
    "    - ADASYN\n",
    "- Undersampling\n",
    "    - RandomUnderSampler\n",
    "    - NearMiss\n",
    "    - CondensedNearestNeighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "X2y2 = X1y1.copy()\n",
    "\n",
    "# Standardize Temperature & Sound\n",
    "std_scaler = StandardScaler()\n",
    "std_features = ['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound']\n",
    "X2y2[std_features] = std_scaler.fit_transform(X1y1[std_features])\n",
    "\n",
    "# Min-Max Scale Light sensors\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_features = ['S1_Light', 'S2_Light', 'S3_Light', 'S4_Light']\n",
    "X2y2[minmax_features] = minmax_scaler.fit_transform(X1y1[minmax_features])\n",
    "\n",
    "# Robust Scale CO2 & CO2 Slope\n",
    "robust_scaler = RobustScaler()\n",
    "robust_features = ['S5_CO2', 'S5_CO2_Slope']\n",
    "X2y2[robust_features] = robust_scaler.fit_transform(X1y1[robust_features])\n",
    "\n",
    "# No standardization necessary for PIR sensor data, since it is already constrained [0, 1]\n",
    "\n",
    "X2y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, CondensedNearestNeighbour\n",
    "\n",
    "X2 = X2y2.drop(columns=[\"Room_Occupancy_Count\"])\n",
    "y2 = X2y2[[\"Room_Occupancy_Count\"]]\n",
    "\n",
    "sampler_tests = {}\n",
    "\n",
    "# Baseline\n",
    "sampler_tests[\"Baseline\"] = [X2, y2]\n",
    "\n",
    "# RandomOverSampler\n",
    "sampler = RandomOverSampler(sampling_strategy=\"auto\", random_state=42)\n",
    "sampler_tests[\"RandomOverSampler\"] = sampler.fit_resample(X2, y2)\n",
    "\n",
    "# SMOTE\n",
    "sampler = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "sampler_tests[\"SMOTE\"] = sampler.fit_resample(X2, y2)\n",
    "\n",
    "# SMOTENC - Categorical features [S6_PIR, S7_PIR]\n",
    "sampler = SMOTENC(sampling_strategy=\"auto\", random_state=42, categorical_features=[14, 15])\n",
    "sampler_tests[\"SMOTENC\"] = sampler.fit_resample(X2, y2)\n",
    "\n",
    "# RandomUnderSampler\n",
    "sampler = RandomUnderSampler(sampling_strategy=\"auto\", random_state=42)\n",
    "sampler_tests[\"RandomUnderSampler\"] = sampler.fit_resample(X2, y2)\n",
    "\n",
    "# NearMiss\n",
    "sampler = NearMiss(sampling_strategy=\"auto\")\n",
    "sampler_tests[\"NearMiss\"] = sampler.fit_resample(X2, y2)\n",
    "\n",
    "# CondensedNearestNeighbour\n",
    "sampler = CondensedNearestNeighbour(sampling_strategy=\"auto\", random_state=42)\n",
    "sampler_tests[\"CondensedNearestNeighbour\"] = sampler.fit_resample(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sampling method, perform tests with different correlation thresholds to determine which features can be disregarded\n",
    "\n",
    "test_thresholds = [0.00, 0.50, 0.65, 0.80]\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for test_method, (X3, y3) in sampler_tests.items():\n",
    "    for threshold in test_thresholds:\n",
    "        X3y3 = X3.join(y3)\n",
    "        corr_matrix = X3y3.corr()\n",
    "\n",
    "        # Find features that have at least one correlation above the threshold (excluding the diagonal); abs() is used to find most and least correlated features.\n",
    "        correlated_features = corr_matrix.abs().columns[(corr_matrix > threshold).sum() > 1]\n",
    "        X4y4 = X3y3[correlated_features]\n",
    "        \n",
    "        test_d = {\n",
    "            method: test_method,\n",
    "            threshold: threshold,\n",
    "            data: X4y4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also represent the data with a pairplot in order to better visualize the distribution of dataset samples, which can be useful to later establish a correlation threshold\n",
    "\n",
    "# For representation purposes, we can reduce the number of datapoints by random sampling.\n",
    "# X2y2_subsampled = X2y2.sample(frac=0.1, random_state=42)  # Take 10% of the data.\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# sns.pairplot(X2y2, hue=\"Room_Occupancy_Count\", diag_kind=\"hist\", palette=\"coolwarm\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data\n",
    "\n",
    "- Removal of columns with lower correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a correlation threshold.\n",
    "threshold = 0.6\n",
    "\n",
    "# Find features that have at least one correlation above the threshold (excluding the diagonal); abs() is used to find most and least correlated features.\n",
    "correlated_features = corr_matrix.abs().columns[(corr_matrix > threshold).sum() > 1]\n",
    "\n",
    "X3y3 = X2y2[correlated_features]\n",
    "\n",
    "print(\"Selected features:\", correlated_features.tolist())\n",
    "\n",
    "X3y3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now normalize the data as is required for our model training method (KNN), even though this is unnecessary for methods like decision trees. **(ChatGPT was used to help select the best normalization technique based on the dataset summary)**:\n",
    "- Temperature values (S1_Temp to S4_Temp):\n",
    "    - Mean is around 25-26°C with a small standard deviation (~0.3 - 0.8).\n",
    "    - The range is small (around 24.9 to 29.0).\n",
    "    - Standardization (Z-score) would work well here.\n",
    "\n",
    "- Light sensor values (S1_Light to S3_Light):\n",
    "    - Huge variation (e.g., S2_Light ranges from 0 to 258).\n",
    "    - Min-Max Scaling is a good option since it preserves relative differences and brings everything between 0 and 1.\n",
    "\n",
    "- Sound sensor values (S3_Sound, S4_Sound):\n",
    "    - Small values (0.04 - 3.67 V) but with a high standard deviation (~0.6).\n",
    "    - Standardization (Z-score) is ideal here.\n",
    "\n",
    "- CO2 (S5_CO2) and CO2 Slope (S5_CO2_Slope):\n",
    "    - Large range in CO2 (345 - 1270 PPM) and very high standard deviation (~250).\n",
    "    - CO2 slope has negative values and a large range (-6.29 to 8.98).\n",
    "    - Robust Scaling is best to reduce the effect of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_dtc = DecisionTreeClassifier()\n",
    "model_knn = KNeighborsClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1y1.drop(columns=['Room_Occupancy_Count']), X1y1['Room_Occupancy_Count'], test_size=0.3, random_state=123)\n",
    "\n",
    "model_dtc.fit(X_train, y_train)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_dtc = model_dtc.predict(X_train)\n",
    "y_pred_knn = model_knn.predict(X_train)\n",
    "\n",
    "#\n",
    "\n",
    "accuracy_dtc = accuracy_score(y_train, y_pred_dtc)\n",
    "accuracy_knn = accuracy_score(y_train, y_pred_knn)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dtc * 100:.2f}%\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn * 100:.2f}%\")\n",
    "\n",
    "y_pred_dtc = model_dtc.predict(X_test)\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "accuracy_dtc = accuracy_score(y_test, y_pred_dtc)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Decision Tree Accuracy: {accuracy_dtc * 100:.2f}%\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0lozUQb04uqOTjFEy2kRL",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
